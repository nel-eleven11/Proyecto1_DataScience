{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1fe888",
   "metadata": {},
   "source": [
    "# Carga de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68500dc5",
   "metadata": {},
   "source": [
    "## .xls / HTML a .csv\n",
    "Los datos del Mineduc se exportan como archivos .xls, realmente teniendo contenido de HTML. Debido a eso, debemos de parsear el archivo HTML e identificar la tabla correcta que contiene los datos. Luego, podemos exportar los datos a su archivo .csv correspondiente, utilizando el valor de la columna 'Departamento' para nombrarlo. Adicionalmente, los archivos utilizan un encoding diferente al estándar utf-8, por lo cual vamos a especificarlo al momento de leer los archivos HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a36f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single file test successful!\n",
      "\n",
      "============================================================\n",
      "PROCESSING ALL FILES\n",
      "============================================================\n",
      "Found 23 .xls files to process\n",
      "  ✅ Processed establecimiento (10).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (11).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (12).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (13).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (14).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (15).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (16).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (17).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (18).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (19).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (1).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (20).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (21).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (22).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (2).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (3).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (4).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (5).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (6).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (7).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (8).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (9).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento.xls: Created 1 CSVs.\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total files: 23\n",
      "✅ Successful: 23\n",
      "❌ Failed: 0\n",
      "\n",
      "✅ No duplicate departamentos found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_html_excel_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    try:\n",
    "        tables = pd.read_html(str(file_path), encoding=\"iso-8859-1\")\n",
    "\n",
    "        if not tables:\n",
    "            return {\"success\": False, \"error\": \"No tables found in HTML\"}\n",
    "\n",
    "        required_headers = [\"CODIGO\", \"DISTRITO\", \"DEPARTAMENTO\", \"MUNICIPIO\"]\n",
    "        target_table = None\n",
    "        target_index = None\n",
    "\n",
    "        for i, df in enumerate(tables):\n",
    "            df_columns_upper = [str(col).upper().strip() for col in df.columns]\n",
    "            if all(header in df_columns_upper for header in required_headers):\n",
    "                target_table = df\n",
    "                target_index = i\n",
    "                break\n",
    "            else:\n",
    "                if len(df) > 0:\n",
    "                    first_row_upper = [\n",
    "                        str(cell).upper().strip() for cell in df.iloc[0]\n",
    "                    ]\n",
    "                    if all(header in first_row_upper for header in required_headers):\n",
    "                        df.columns = df.iloc[0]\n",
    "                        df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "                        target_table = df\n",
    "                        target_index = i\n",
    "                        break\n",
    "\n",
    "        if target_table is None:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"No table found with required headers.\",\n",
    "            }\n",
    "\n",
    "        target_table = target_table.dropna(how=\"all\")\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"data\": target_table,\n",
    "            \"table_index\": target_index,\n",
    "            \"total_tables\": len(tables),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def sanitize_filename(text):\n",
    "    filename = text.lower().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^\\w_.]\", \"\", filename)\n",
    "\n",
    "\n",
    "def process_html_files_directory(input_dir, output_dir):\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(input_path.glob(\"*.xls\"))\n",
    "\n",
    "    print(f\"Found {len(files)} .xls files to process\")\n",
    "\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    all_departamentos = defaultdict(list)\n",
    "\n",
    "    for file_path in files:\n",
    "        result = parse_html_excel_file(file_path)\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            df = result[\"data\"]\n",
    "\n",
    "            departamentos = []\n",
    "            for departamento, group in df.groupby(\"DEPARTAMENTO\"):\n",
    "                filename = f\"datos_{sanitize_filename(departamento)}.csv\"\n",
    "\n",
    "                output_file = output_path / filename\n",
    "                group.to_csv(output_file, index=False)\n",
    "\n",
    "                departamentos.append(\n",
    "                    {\"name\": departamento, \"filename\": filename, \"rows\": len(group)}\n",
    "                )\n",
    "\n",
    "                all_departamentos[departamento].append(\n",
    "                    {\n",
    "                        \"source_file\": file_path.name,\n",
    "                        \"csv_file\": filename,\n",
    "                        \"rows\": len(group),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            successful_files.append(\n",
    "                {\n",
    "                    \"file\": file_path.name,\n",
    "                    \"departamentos\": departamentos,\n",
    "                    \"total_rows\": len(df),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"  ✅ Processed {file_path.name}: Created {len(departamentos)} CSVs.\")\n",
    "\n",
    "        else:\n",
    "            failed_files.append({\"file\": file_path.name, \"error\": result[\"error\"]})\n",
    "            print(f\"  ❌ Failed {file_path.name}: {result['error']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nTotal files: {len(files)}\")\n",
    "    print(f\"✅ Successful: {len(successful_files)}\")\n",
    "    print(f\"❌ Failed: {len(failed_files)}\")\n",
    "\n",
    "    # The detailed successful/failed files lists will only be printed if there are successful/failed files.\n",
    "    # The summary already gives a count, so the repetition for successful files is removed here.\n",
    "    # The detailed list for failed files remains as it provides useful error messages.\n",
    "    if failed_files:\n",
    "        print(f\"\\n❌ FAILED FILES:\")\n",
    "        for item in failed_files:\n",
    "            print(f\"  - {item['file']}: {item['error']}\")\n",
    "\n",
    "    duplicates = {\n",
    "        name: sources\n",
    "        for name, sources in all_departamentos.items()\n",
    "        if len(sources) > 1\n",
    "    }\n",
    "    if duplicates:\n",
    "        print(f\"\\n⚠️ DUPLICATE DEPARTAMENTOS:\")\n",
    "        for dept_name, sources in duplicates.items():\n",
    "            print(f\"  {dept_name}: appears in {len(sources)} files\")\n",
    "    else:\n",
    "        print(\"\\n✅ No duplicate departamentos found\")\n",
    "\n",
    "\n",
    "test_result = parse_html_excel_file(\"data/raw/establecimiento.xls\")\n",
    "if test_result[\"success\"]:\n",
    "    print(\"\\nSingle file test successful!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING ALL FILES\")\n",
    "    print(\"=\" * 60)\n",
    "    process_html_files_directory(\"data/raw\", \"data/csv\")\n",
    "else:\n",
    "    print(f\"❌ Single file test failed: {test_result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e96a1",
   "metadata": {},
   "source": [
    "## .csv a DataFrames\n",
    "Luego de haber creado los archivos, podemos cargarlos a DataFrames para realizar el análisis necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89bd3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 CSV files\n",
      "Loaded 23 datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load all CSV files\n",
    "csv_dir = Path(\"data/csv\")\n",
    "csv_files = list(csv_dir.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {}\n",
    "for csv_file in csv_files:\n",
    "    dataset_name = csv_file.stem\n",
    "    datasets[dataset_name] = pd.read_csv(csv_file)\n",
    "    \n",
    "print(f\"Loaded {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9176a2",
   "metadata": {},
   "source": [
    "# Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad22cb",
   "metadata": {},
   "source": [
    "## Filas y Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09701424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset Shapes:\n",
      "                 dataset  rows  columns\n",
      "19       datos_guatemala  1036       17\n",
      "16  datos_ciudad_capital   864       17\n",
      "7       datos_san_marcos   431       17\n",
      "18       datos_escuintla   393       17\n",
      "3   datos_quetzaltenango   365       17\n",
      "14   datos_chimaltenango   300       17\n",
      "1          datos_jutiapa   296       17\n",
      "11   datos_suchitepequez   296       17\n",
      "20   datos_huehuetenango   295       17\n",
      "22    datos_alta_verapaz   294       17\n",
      "21          datos_izabal   273       17\n",
      "5       datos_retalhuleu   272       17\n",
      "2            datos_peten   270       17\n",
      "6     datos_sacatepequez   208       17\n",
      "4           datos_quiche   184       17\n",
      "15      datos_chiquimula   136       17\n",
      "8       datos_santa_rosa   133       17\n",
      "0           datos_jalapa   121       17\n",
      "9           datos_solola   111       17\n",
      "17     datos_el_progreso    97       17\n",
      "10    datos_baja_verapaz    94       17\n",
      "13          datos_zacapa    70       17\n",
      "12     datos_totonicapan    51       17\n",
      "\n",
      "📈 Summary:\n",
      "Total rows across all datasets: 6,590\n",
      "Average rows per dataset: 287\n",
      "Min/Max rows: 51 / 1036\n"
     ]
    }
   ],
   "source": [
    "# Check shapes of all datasets\n",
    "shape_info = []\n",
    "for name, df in datasets.items():\n",
    "    shape_info.append({\n",
    "        'dataset': name,\n",
    "        'rows': df.shape[0],\n",
    "        'columns': df.shape[1]\n",
    "    })\n",
    "\n",
    "shape_df = pd.DataFrame(shape_info)\n",
    "print(\"📊 Dataset Shapes:\")\n",
    "print(shape_df.sort_values('rows', ascending=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📈 Summary:\")\n",
    "print(f\"Total rows across all datasets: {shape_df['rows'].sum():,}\")\n",
    "print(f\"Average rows per dataset: {shape_df['rows'].mean():.0f}\")\n",
    "print(f\"Min/Max rows: {shape_df['rows'].min()} / {shape_df['rows'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd8fd4",
   "metadata": {},
   "source": [
    "## Integridad de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fef65",
   "metadata": {},
   "source": [
    "### Consistencia en Nombres de Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18e31a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Standard columns (17):\n",
      "   1. CODIGO\n",
      "   2. DISTRITO\n",
      "   3. DEPARTAMENTO\n",
      "   4. MUNICIPIO\n",
      "   5. ESTABLECIMIENTO\n",
      "   6. DIRECCION\n",
      "   7. TELEFONO\n",
      "   8. SUPERVISOR\n",
      "   9. DIRECTOR\n",
      "  10. NIVEL\n",
      "  11. SECTOR\n",
      "  12. AREA\n",
      "  13. STATUS\n",
      "  14. MODALIDAD\n",
      "  15. JORNADA\n",
      "  16. PLAN\n",
      "  17. DEPARTAMENTAL\n"
     ]
    }
   ],
   "source": [
    "# Check if all datasets have the same columns\n",
    "all_columns = []\n",
    "column_consistency = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    columns = list(df.columns)\n",
    "    all_columns.append(columns)\n",
    "    column_consistency[name] = columns\n",
    "\n",
    "# Check if all column sets are identical\n",
    "first_columns = all_columns[0]\n",
    "all_same = all(columns == first_columns for columns in all_columns)\n",
    "\n",
    "if all_same:\n",
    "    print(f\"\\n✅ Standard columns ({len(first_columns)}):\")\n",
    "    for i, col in enumerate(first_columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "else:\n",
    "    print(\"\\n❌ Column differences found:\")\n",
    "    for name, columns in column_consistency.items():\n",
    "        if columns != first_columns:\n",
    "            print(f\"  {name}: {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54d5d4",
   "metadata": {},
   "source": [
    "### Encoding Problemático\n",
    "Como mencionamos anteriormente, el encoding de los archivos originales era distinto de \"utf-8\". Nos dimos cuenta al realizar el análisis sobre el encoding problemático, sin embargo al cambiarlo dentro de la función anterior logramos correr con éxito este análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "330170f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No encoding issues found (char: '�')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Check for encoding issues (corrupted characters)\n",
    "problematic_char = \"�\"\n",
    "all_problematic_samples = defaultdict(list)\n",
    "issue_found = False\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":  # Only check text columns\n",
    "            str_series = df[col].astype(str) \n",
    "            \n",
    "            # Identify rows with the problematic character '�'\n",
    "            contains_char_mask = str_series.str.contains(problematic_char, na=False)\n",
    "            \n",
    "            if contains_char_mask.any(): # If any '�' found in this column\n",
    "                issue_found = True\n",
    "                # Get unique problematic values, up to 5, for this column\n",
    "                current_samples = str_series[contains_char_mask].unique().tolist()\n",
    "                for sample_val in current_samples:\n",
    "                    if len(all_problematic_samples[col]) < 5:\n",
    "                        all_problematic_samples[col].append(sample_val)\n",
    "\n",
    "\n",
    "if issue_found:\n",
    "    print(f\"❌ Encoding issues found (char: '{problematic_char}'):\")\n",
    "    # Sort columns by name for consistent output\n",
    "    sorted_cols_with_issues = sorted(all_problematic_samples.keys()) \n",
    "\n",
    "    for col in sorted_cols_with_issues:\n",
    "        samples = all_problematic_samples[col]\n",
    "        print(f\"  Column '{col}':\")\n",
    "        for val in samples:\n",
    "            print(f\"    • {val}\")\n",
    "else:\n",
    "    print(f\"✅ No encoding issues found (char: '{problematic_char}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d679f59",
   "metadata": {},
   "source": [
    "# Análisis de Variables\n",
    "Las variables que más operaciones de limpieza necesitan son:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8735b0",
   "metadata": {},
   "source": [
    "## Código\n",
    "Este valor parece ser un identificador único, queremos explorar las siguientes propiedades:\n",
    "- Unicidad: Este código es único dentro de su respectivo dataset o todos?\n",
    "- Formato: Es consistente el formato en todos los datasets? Existen errores de digitación?\n",
    "- Valores Faltantes: Existen valores faltantes?\n",
    "- Nos ayuda a identificar únicamente algún otro valor?\n",
    "\n",
    "Debido a esto, queremos realizar los siguientes pasos de limpieza:\n",
    "\n",
    "- Identificar unicidad del código\n",
    "- Identificar Valores Faltantes\n",
    "- Revisar errores de digitación, cómo lo pueden ser whitespaces o formato incongruente\n",
    "- Identificar si nos puede ayudar a identificar otras variables para verificar errores de digitación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4fee8",
   "metadata": {},
   "source": [
    "## Distrito\n",
    "Este valor parece ser un identificador geográfico, siguiendo un formato XX-YYY. Queremos explorar las siguientes propiedades\n",
    "- Formato: Es consistente el formato?\n",
    "- Valores Faltantes: Existen valores faltantes dentro de los datasets?\n",
    "- Nos ayuda a identificar únicamente algún otro valor?\n",
    "\n",
    "Debido a esto, queremos realizar los siguientes pasos de limpieza:\n",
    "\n",
    "- Identificar valores faltantes\n",
    "- Revisar errores de digitación\n",
    "- Enforzar un formato consistente\n",
    "- Identificar si nos puede ayuda a verificar la consistencia de Municipio o algún otro valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5415d62",
   "metadata": {},
   "source": [
    "## Departamento\n",
    "\n",
    "Ya que los DataFrames se encuentran divididos por departamento, esta entrada debería ser completamente consistente. Además, podemos proponer los siguientes pasos para una mayor consistencia:\n",
    "\n",
    "- Identificar el valor RAW más común, ya que un error de digitación sería menos frecuente\n",
    "- Tomar ese valor y realizar las siguientes transformaciones\n",
    "    - Conversión a minúsculas\n",
    "    - Reemplazo de espacios por _\n",
    "    - Aplicar a todas las columnas de cada DF individual\n",
    "- Aplicar OHE luego de mergear los DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e2a71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ddb656",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
