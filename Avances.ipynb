{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1fe888",
   "metadata": {},
   "source": [
    "# Carga de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68500dc5",
   "metadata": {},
   "source": [
    "## .xls / HTML a .csv\n",
    "Los datos del Mineduc se exportan como archivos .xls, realmente teniendo contenido de HTML. Debido a eso, debemos de parsear el archivo HTML e identificar la tabla correcta que contiene los datos. Luego, podemos exportar los datos a su archivo .csv correspondiente, utilizando el valor de la columna 'Departamento' para nombrarlo. Adicionalmente, los archivos utilizan un encoding diferente al est√°ndar utf-8, por lo cual vamos a especificarlo al momento de leer los archivos HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a36f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single file test successful!\n",
      "\n",
      "============================================================\n",
      "PROCESSING ALL FILES\n",
      "============================================================\n",
      "Found 23 .xls files to process\n",
      "  ‚úÖ Processed establecimiento (10).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (11).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (12).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (13).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (14).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (15).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (16).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (17).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (18).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (19).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (1).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (20).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (21).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (22).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (2).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (3).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (4).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (5).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (6).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (7).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (8).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento (9).xls: Created 1 CSVs.\n",
      "  ‚úÖ Processed establecimiento.xls: Created 1 CSVs.\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total files: 23\n",
      "‚úÖ Successful: 23\n",
      "‚ùå Failed: 0\n",
      "\n",
      "‚úÖ No duplicate departamentos found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_html_excel_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    try:\n",
    "        tables = pd.read_html(str(file_path), encoding=\"iso-8859-1\")\n",
    "\n",
    "        if not tables:\n",
    "            return {\"success\": False, \"error\": \"No tables found in HTML\"}\n",
    "\n",
    "        required_headers = [\"CODIGO\", \"DISTRITO\", \"DEPARTAMENTO\", \"MUNICIPIO\"]\n",
    "        target_table = None\n",
    "        target_index = None\n",
    "\n",
    "        for i, df in enumerate(tables):\n",
    "            df_columns_upper = [str(col).upper().strip() for col in df.columns]\n",
    "            if all(header in df_columns_upper for header in required_headers):\n",
    "                target_table = df\n",
    "                target_index = i\n",
    "                break\n",
    "            else:\n",
    "                if len(df) > 0:\n",
    "                    first_row_upper = [\n",
    "                        str(cell).upper().strip() for cell in df.iloc[0]\n",
    "                    ]\n",
    "                    if all(header in first_row_upper for header in required_headers):\n",
    "                        df.columns = df.iloc[0]\n",
    "                        df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "                        target_table = df\n",
    "                        target_index = i\n",
    "                        break\n",
    "\n",
    "        if target_table is None:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"No table found with required headers.\",\n",
    "            }\n",
    "\n",
    "        target_table = target_table.dropna(how=\"all\")\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"data\": target_table,\n",
    "            \"table_index\": target_index,\n",
    "            \"total_tables\": len(tables),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def sanitize_filename(text):\n",
    "    filename = text.lower().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^\\w_.]\", \"\", filename)\n",
    "\n",
    "\n",
    "def process_html_files_directory(input_dir, output_dir):\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(input_path.glob(\"*.xls\"))\n",
    "\n",
    "    print(f\"Found {len(files)} .xls files to process\")\n",
    "\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    all_departamentos = defaultdict(list)\n",
    "\n",
    "    for file_path in files:\n",
    "        result = parse_html_excel_file(file_path)\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            df = result[\"data\"]\n",
    "\n",
    "            departamentos = []\n",
    "            for departamento, group in df.groupby(\"DEPARTAMENTO\"):\n",
    "                filename = f\"datos_{sanitize_filename(departamento)}.csv\"\n",
    "\n",
    "                output_file = output_path / filename\n",
    "                group.to_csv(output_file, index=False)\n",
    "\n",
    "                departamentos.append(\n",
    "                    {\"name\": departamento, \"filename\": filename, \"rows\": len(group)}\n",
    "                )\n",
    "\n",
    "                all_departamentos[departamento].append(\n",
    "                    {\n",
    "                        \"source_file\": file_path.name,\n",
    "                        \"csv_file\": filename,\n",
    "                        \"rows\": len(group),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            successful_files.append(\n",
    "                {\n",
    "                    \"file\": file_path.name,\n",
    "                    \"departamentos\": departamentos,\n",
    "                    \"total_rows\": len(df),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"  ‚úÖ Processed {file_path.name}: Created {len(departamentos)} CSVs.\")\n",
    "\n",
    "        else:\n",
    "            failed_files.append({\"file\": file_path.name, \"error\": result[\"error\"]})\n",
    "            print(f\"  ‚ùå Failed {file_path.name}: {result['error']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nTotal files: {len(files)}\")\n",
    "    print(f\"‚úÖ Successful: {len(successful_files)}\")\n",
    "    print(f\"‚ùå Failed: {len(failed_files)}\")\n",
    "\n",
    "    # The detailed successful/failed files lists will only be printed if there are successful/failed files.\n",
    "    # The summary already gives a count, so the repetition for successful files is removed here.\n",
    "    # The detailed list for failed files remains as it provides useful error messages.\n",
    "    if failed_files:\n",
    "        print(f\"\\n‚ùå FAILED FILES:\")\n",
    "        for item in failed_files:\n",
    "            print(f\"  - {item['file']}: {item['error']}\")\n",
    "\n",
    "    duplicates = {\n",
    "        name: sources\n",
    "        for name, sources in all_departamentos.items()\n",
    "        if len(sources) > 1\n",
    "    }\n",
    "    if duplicates:\n",
    "        print(f\"\\n‚ö†Ô∏è DUPLICATE DEPARTAMENTOS:\")\n",
    "        for dept_name, sources in duplicates.items():\n",
    "            print(f\"  {dept_name}: appears in {len(sources)} files\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No duplicate departamentos found\")\n",
    "\n",
    "\n",
    "test_result = parse_html_excel_file(\"data/raw/establecimiento.xls\")\n",
    "if test_result[\"success\"]:\n",
    "    print(\"\\nSingle file test successful!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING ALL FILES\")\n",
    "    print(\"=\" * 60)\n",
    "    process_html_files_directory(\"data/raw\", \"data/csv\")\n",
    "else:\n",
    "    print(f\"‚ùå Single file test failed: {test_result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e96a1",
   "metadata": {},
   "source": [
    "## .csv a DataFrames\n",
    "Luego de haber creado los archivos, podemos cargarlos a DataFrames para realizar el an√°lisis necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89bd3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 CSV files\n",
      "Loaded 23 datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load all CSV files\n",
    "csv_dir = Path(\"data/csv\")\n",
    "csv_files = list(csv_dir.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {}\n",
    "for csv_file in csv_files:\n",
    "    dataset_name = csv_file.stem\n",
    "    datasets[dataset_name] = pd.read_csv(csv_file)\n",
    "    \n",
    "print(f\"Loaded {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9176a2",
   "metadata": {},
   "source": [
    "# Descripci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad22cb",
   "metadata": {},
   "source": [
    "## Filas y Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09701424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Shapes:\n",
      "                 dataset  rows  columns\n",
      "19       datos_guatemala  1036       17\n",
      "16  datos_ciudad_capital   864       17\n",
      "7       datos_san_marcos   431       17\n",
      "18       datos_escuintla   393       17\n",
      "3   datos_quetzaltenango   365       17\n",
      "14   datos_chimaltenango   300       17\n",
      "1          datos_jutiapa   296       17\n",
      "11   datos_suchitepequez   296       17\n",
      "20   datos_huehuetenango   295       17\n",
      "22    datos_alta_verapaz   294       17\n",
      "21          datos_izabal   273       17\n",
      "5       datos_retalhuleu   272       17\n",
      "2            datos_peten   270       17\n",
      "6     datos_sacatepequez   208       17\n",
      "4           datos_quiche   184       17\n",
      "15      datos_chiquimula   136       17\n",
      "8       datos_santa_rosa   133       17\n",
      "0           datos_jalapa   121       17\n",
      "9           datos_solola   111       17\n",
      "17     datos_el_progreso    97       17\n",
      "10    datos_baja_verapaz    94       17\n",
      "13          datos_zacapa    70       17\n",
      "12     datos_totonicapan    51       17\n",
      "\n",
      "üìà Summary:\n",
      "Total rows across all datasets: 6,590\n",
      "Average rows per dataset: 287\n",
      "Min/Max rows: 51 / 1036\n"
     ]
    }
   ],
   "source": [
    "# Check shapes of all datasets\n",
    "shape_info = []\n",
    "for name, df in datasets.items():\n",
    "    shape_info.append({\n",
    "        'dataset': name,\n",
    "        'rows': df.shape[0],\n",
    "        'columns': df.shape[1]\n",
    "    })\n",
    "\n",
    "shape_df = pd.DataFrame(shape_info)\n",
    "print(\"üìä Dataset Shapes:\")\n",
    "print(shape_df.sort_values('rows', ascending=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà Summary:\")\n",
    "print(f\"Total rows across all datasets: {shape_df['rows'].sum():,}\")\n",
    "print(f\"Average rows per dataset: {shape_df['rows'].mean():.0f}\")\n",
    "print(f\"Min/Max rows: {shape_df['rows'].min()} / {shape_df['rows'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd8fd4",
   "metadata": {},
   "source": [
    "## Integridad de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fef65",
   "metadata": {},
   "source": [
    "### Consistencia en Nombres de Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18e31a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Standard columns (17):\n",
      "   1. CODIGO\n",
      "   2. DISTRITO\n",
      "   3. DEPARTAMENTO\n",
      "   4. MUNICIPIO\n",
      "   5. ESTABLECIMIENTO\n",
      "   6. DIRECCION\n",
      "   7. TELEFONO\n",
      "   8. SUPERVISOR\n",
      "   9. DIRECTOR\n",
      "  10. NIVEL\n",
      "  11. SECTOR\n",
      "  12. AREA\n",
      "  13. STATUS\n",
      "  14. MODALIDAD\n",
      "  15. JORNADA\n",
      "  16. PLAN\n",
      "  17. DEPARTAMENTAL\n"
     ]
    }
   ],
   "source": [
    "# Check if all datasets have the same columns\n",
    "all_columns = []\n",
    "column_consistency = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    columns = list(df.columns)\n",
    "    all_columns.append(columns)\n",
    "    column_consistency[name] = columns\n",
    "\n",
    "# Check if all column sets are identical\n",
    "first_columns = all_columns[0]\n",
    "all_same = all(columns == first_columns for columns in all_columns)\n",
    "\n",
    "if all_same:\n",
    "    print(f\"\\n‚úÖ Standard columns ({len(first_columns)}):\")\n",
    "    for i, col in enumerate(first_columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Column differences found:\")\n",
    "    for name, columns in column_consistency.items():\n",
    "        if columns != first_columns:\n",
    "            print(f\"  {name}: {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54d5d4",
   "metadata": {},
   "source": [
    "### Encoding Problem√°tico\n",
    "Como mencionamos anteriormente, el encoding de los archivos originales era distinto de \"utf-8\". Nos dimos cuenta al realizar el an√°lisis sobre el encoding problem√°tico, sin embargo al cambiarlo dentro de la funci√≥n anterior logramos correr con √©xito este an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "330170f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No encoding issues found (char: 'ÔøΩ')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Check for encoding issues (corrupted characters)\n",
    "problematic_char = \"ÔøΩ\"\n",
    "all_problematic_samples = defaultdict(list)\n",
    "issue_found = False\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":  # Only check text columns\n",
    "            str_series = df[col].astype(str) \n",
    "            \n",
    "            # Identify rows with the problematic character 'ÔøΩ'\n",
    "            contains_char_mask = str_series.str.contains(problematic_char, na=False)\n",
    "            \n",
    "            if contains_char_mask.any(): # If any 'ÔøΩ' found in this column\n",
    "                issue_found = True\n",
    "                # Get unique problematic values, up to 5, for this column\n",
    "                current_samples = str_series[contains_char_mask].unique().tolist()\n",
    "                for sample_val in current_samples:\n",
    "                    if len(all_problematic_samples[col]) < 5:\n",
    "                        all_problematic_samples[col].append(sample_val)\n",
    "\n",
    "\n",
    "if issue_found:\n",
    "    print(f\"‚ùå Encoding issues found (char: '{problematic_char}'):\")\n",
    "    # Sort columns by name for consistent output\n",
    "    sorted_cols_with_issues = sorted(all_problematic_samples.keys()) \n",
    "\n",
    "    for col in sorted_cols_with_issues:\n",
    "        samples = all_problematic_samples[col]\n",
    "        print(f\"  Column '{col}':\")\n",
    "        for val in samples:\n",
    "            print(f\"    ‚Ä¢ {val}\")\n",
    "else:\n",
    "    print(f\"‚úÖ No encoding issues found (char: '{problematic_char}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d679f59",
   "metadata": {},
   "source": [
    "# An√°lisis de Variables\n",
    "Las variables que m√°s operaciones de limpieza necesitan son:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8735b0",
   "metadata": {},
   "source": [
    "## C√≥digo\n",
    "Este valor parece ser un identificador √∫nico, queremos explorar las siguientes propiedades:\n",
    "- Unicidad: Este c√≥digo es √∫nico dentro de su respectivo dataset o todos?\n",
    "- Formato: Es consistente el formato en todos los datasets? Existen errores de digitaci√≥n?\n",
    "- Valores Faltantes: Existen valores faltantes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4500e",
   "metadata": {},
   "source": [
    "Primero, podemos verificar si es necesario realizar transformaciones para evitar errores de digitaci√≥n. Utilizando Regex, verificamos que siga un patr√≥n estricto para garantizar consistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f59ea288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Todos los c√≥digos cumplen con el formato estricto (XX-YY-ZZZZ-AA) y no tienen espacios en blanco iniciales/finales.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def verify_codigo_format_only(datasets: dict):\n",
    "\n",
    "    strict_codigo_pattern = re.compile(r'^\\d{1,2}-\\d{1,3}-\\d{1,4}-\\d{1,3}$')\n",
    "\n",
    "    format_issues_by_dataset = defaultdict(list)\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "\n",
    "        # Use the raw string representation without stripping\n",
    "        codigo_series_raw = df['CODIGO'].astype(str)\n",
    "\n",
    "        # Iterate over non-null codes to check format and spaces\n",
    "        for idx, code in codigo_series_raw.dropna().items():\n",
    "            if not strict_codigo_pattern.match(code):\n",
    "                # If it doesn't match the strict pattern at all\n",
    "                format_issues_by_dataset[name].append(f\"'{code}' (fila {idx}) - No coincide con el formato estricto (XX-YY-ZZZZ-AA)\")\n",
    "            elif code != code.strip():\n",
    "                # If it matches the pattern but has leading/trailing spaces\n",
    "                format_issues_by_dataset[name].append(f\"'{code}' (fila {idx}) - Contiene espacios en blanco al inicio/final\")\n",
    "\n",
    "    if not format_issues_by_dataset:\n",
    "        print(\"  ‚úÖ Todos los c√≥digos cumplen con el formato estricto (XX-YY-ZZZZ-AA) y no tienen espacios en blanco iniciales/finales.\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Se encontraron problemas de formato o espacios en blanco:\")\n",
    "        for name, issues in format_issues_by_dataset.items():\n",
    "            print(f\"\\nDataset: '{name}' ({len(issues)} problemas)\")\n",
    "            for i, issue in enumerate(issues[:10]):\n",
    "                print(f\"  ‚Ä¢ {issue}\")\n",
    "            if len(issues) > 10:\n",
    "                print(f\"  ... y {len(issues) - 10} problemas m√°s.\")\n",
    "    \n",
    "\n",
    "verify_codigo_format_only(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e387c",
   "metadata": {},
   "source": [
    "Luego, podemos verificar valores faltantes, entradas duplicadas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8bce2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resumen de Limpieza de 'CODIGO' por Dataset ---\n",
      "\n",
      "Dataset: 'datos_jalapa'\n",
      "  ‚Ä¢ Total entradas (original): 121\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 121\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'21': 121}\n",
      "    - segment_2: {'01': 63, '07': 17, '03': 12, '06': 11, '05': 8}\n",
      "    - segment_3: {'0111': 2, '0036': 2, '0052': 2, '0054': 2, '0101': 1}\n",
      "    - segment_4: {'46': 121}\n",
      "\n",
      "Dataset: 'datos_jutiapa'\n",
      "  ‚Ä¢ Total entradas (original): 296\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 296\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'22': 296}\n",
      "    - segment_2: {'01': 90, '12': 28, '05': 22, '14': 21, '11': 18}\n",
      "    - segment_3: {'0024': 6, '0031': 6, '0023': 5, '0026': 5, '0011': 4}\n",
      "    - segment_4: {'46': 296}\n",
      "\n",
      "Dataset: 'datos_peten'\n",
      "  ‚Ä¢ Total entradas (original): 270\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 270\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'17': 270}\n",
      "    - segment_2: {'05': 44, '03': 36, '10': 32, '01': 28, '12': 23}\n",
      "    - segment_3: {'0124': 4, '0002': 4, '0099': 4, '0156': 3, '0089': 3}\n",
      "    - segment_4: {'46': 270}\n",
      "\n",
      "Dataset: 'datos_quetzaltenango'\n",
      "  ‚Ä¢ Total entradas (original): 365\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 365\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'09': 365}\n",
      "    - segment_2: {'01': 175, '20': 74, '09': 21, '23': 12, '17': 11}\n",
      "    - segment_3: {'0016': 4, '0046': 4, '0067': 4, '0098': 3, '0012': 3}\n",
      "    - segment_4: {'46': 365}\n",
      "\n",
      "Dataset: 'datos_quiche'\n",
      "  ‚Ä¢ Total entradas (original): 184\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 184\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'14': 184}\n",
      "    - segment_2: {'01': 43, '20': 40, '15': 15, '06': 13, '21': 11}\n",
      "    - segment_3: {'0085': 3, '0103': 3, '0068': 3, '0059': 3, '0077': 2}\n",
      "    - segment_4: {'46': 184}\n",
      "\n",
      "Dataset: 'datos_retalhuleu'\n",
      "  ‚Ä¢ Total entradas (original): 272\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 272\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'11': 272}\n",
      "    - segment_2: {'01': 156, '06': 21, '02': 19, '08': 17, '09': 17}\n",
      "    - segment_3: {'0024': 5, '0027': 3, '0081': 3, '0015': 3, '0056': 3}\n",
      "    - segment_4: {'46': 272}\n",
      "\n",
      "Dataset: 'datos_sacatepequez'\n",
      "  ‚Ä¢ Total entradas (original): 208\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 208\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'03': 208}\n",
      "    - segment_2: {'01': 67, '12': 28, '08': 25, '02': 16, '04': 15}\n",
      "    - segment_3: {'0022': 6, '0029': 5, '0057': 4, '0058': 4, '0059': 4}\n",
      "    - segment_4: {'46': 208}\n",
      "\n",
      "Dataset: 'datos_san_marcos'\n",
      "  ‚Ä¢ Total entradas (original): 431\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 431\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'12': 431}\n",
      "    - segment_2: {'15': 67, '01': 44, '02': 35, '04': 30, '29': 23}\n",
      "    - segment_3: {'0027': 6, '0031': 6, '0085': 5, '0043': 5, '0069': 5}\n",
      "    - segment_4: {'46': 431}\n",
      "\n",
      "Dataset: 'datos_santa_rosa'\n",
      "  ‚Ä¢ Total entradas (original): 133\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 133\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'06': 133}\n",
      "    - segment_2: {'02': 26, '08': 25, '14': 16, '01': 15, '06': 9}\n",
      "    - segment_3: {'0030': 5, '0028': 5, '0022': 4, '0037': 3, '0046': 3}\n",
      "    - segment_4: {'46': 133}\n",
      "\n",
      "Dataset: 'datos_solola'\n",
      "  ‚Ä¢ Total entradas (original): 111\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 111\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'07': 111}\n",
      "    - segment_2: {'01': 31, '10': 16, '13': 10, '04': 8, '18': 8}\n",
      "    - segment_3: {'0008': 3, '0027': 3, '0028': 3, '0023': 2, '0041': 2}\n",
      "    - segment_4: {'46': 111}\n",
      "\n",
      "Dataset: 'datos_baja_verapaz'\n",
      "  ‚Ä¢ Total entradas (original): 94\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 94\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'15': 94}\n",
      "    - segment_2: {'01': 28, '02': 16, '03': 15, '08': 12, '04': 8}\n",
      "    - segment_3: {'0111': 2, '0059': 2, '0100': 2, '0131': 2, '0046': 2}\n",
      "    - segment_4: {'46': 94}\n",
      "\n",
      "Dataset: 'datos_suchitepequez'\n",
      "  ‚Ä¢ Total entradas (original): 296\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 296\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'10': 296}\n",
      "    - segment_2: {'01': 126, '10': 25, '14': 23, '17': 17, '21': 16}\n",
      "    - segment_3: {'0025': 6, '0039': 5, '0009': 5, '0031': 4, '0056': 4}\n",
      "    - segment_4: {'46': 296}\n",
      "\n",
      "Dataset: 'datos_totonicapan'\n",
      "  ‚Ä¢ Total entradas (original): 51\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 51\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'08': 51}\n",
      "    - segment_2: {'01': 22, '02': 15, '05': 4, '06': 4, '03': 3}\n",
      "    - segment_3: {'0050': 2, '0061': 2, '0151': 2, '0106': 1, '0107': 1}\n",
      "    - segment_4: {'46': 51}\n",
      "\n",
      "Dataset: 'datos_zacapa'\n",
      "  ‚Ä¢ Total entradas (original): 70\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 70\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'19': 70}\n",
      "    - segment_2: {'01': 30, '04': 8, '05': 7, '09': 5, '06': 4}\n",
      "    - segment_3: {'0004': 2, '0005': 2, '0015': 2, '0052': 2, '0048': 2}\n",
      "    - segment_4: {'46': 70}\n",
      "\n",
      "Dataset: 'datos_chimaltenango'\n",
      "  ‚Ä¢ Total entradas (original): 300\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 300\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'04': 300}\n",
      "    - segment_2: {'01': 136, '03': 27, '04': 24, '06': 19, '12': 13}\n",
      "    - segment_3: {'0037': 4, '0023': 4, '0044': 3, '0059': 3, '0063': 3}\n",
      "    - segment_4: {'46': 300}\n",
      "\n",
      "Dataset: 'datos_chiquimula'\n",
      "  ‚Ä¢ Total entradas (original): 136\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 136\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'20': 136}\n",
      "    - segment_2: {'01': 43, '07': 23, '04': 20, '09': 16, '11': 15}\n",
      "    - segment_3: {'0029': 2, '0030': 2, '0197': 2, '0213': 2, '0256': 2}\n",
      "    - segment_4: {'46': 136}\n",
      "\n",
      "Dataset: 'datos_ciudad_capital'\n",
      "  ‚Ä¢ Total entradas (original): 864\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 864\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'00': 864}\n",
      "    - segment_2: {'01': 300, '07': 109, '12': 78, '18': 70, '19': 43}\n",
      "    - segment_3: {'0049': 6, '0007': 6, '0077': 6, '0016': 5, '0078': 5}\n",
      "    - segment_4: {'46': 864}\n",
      "\n",
      "Dataset: 'datos_el_progreso'\n",
      "  ‚Ä¢ Total entradas (original): 97\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 97\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'02': 97}\n",
      "    - segment_2: {'03': 31, '01': 19, '07': 19, '06': 10, '05': 7}\n",
      "    - segment_3: {'0004': 3, '0014': 3, '0021': 3, '0054': 2, '0022': 2}\n",
      "    - segment_4: {'46': 97}\n",
      "\n",
      "Dataset: 'datos_escuintla'\n",
      "  ‚Ä¢ Total entradas (original): 393\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 393\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'05': 393}\n",
      "    - segment_2: {'01': 97, '02': 74, '06': 34, '09': 33, '13': 31}\n",
      "    - segment_3: {'0039': 5, '0050': 5, '0083': 5, '0067': 4, '0108': 4}\n",
      "    - segment_4: {'46': 393}\n",
      "\n",
      "Dataset: 'datos_guatemala'\n",
      "  ‚Ä¢ Total entradas (original): 1036\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 1036\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'01': 1036}\n",
      "    - segment_2: {'08': 300, '15': 223, '17': 94, '14': 78, '10': 71}\n",
      "    - segment_3: {'0128': 5, '0130': 5, '0136': 5, '0181': 5, '0071': 4}\n",
      "    - segment_4: {'46': 1036}\n",
      "\n",
      "Dataset: 'datos_huehuetenango'\n",
      "  ‚Ä¢ Total entradas (original): 295\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 295\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'13': 295}\n",
      "    - segment_2: {'01': 86, '12': 25, '26': 23, '02': 17, '07': 13}\n",
      "    - segment_3: {'0055': 5, '0075': 5, '0067': 4, '0076': 4, '0041': 4}\n",
      "    - segment_4: {'46': 295}\n",
      "\n",
      "Dataset: 'datos_izabal'\n",
      "  ‚Ä¢ Total entradas (original): 273\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 273\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'18': 273}\n",
      "    - segment_2: {'04': 98, '01': 95, '02': 37, '05': 29, '03': 14}\n",
      "    - segment_3: {'0400': 3, '0404': 3, '0145': 2, '0174': 2, '0185': 2}\n",
      "    - segment_4: {'46': 273}\n",
      "\n",
      "Dataset: 'datos_alta_verapaz'\n",
      "  ‚Ä¢ Total entradas (original): 294\n",
      "  ‚Ä¢ Valores faltantes (despu√©s de limpieza): 0 (0.00%)\n",
      "  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): 294\n",
      "  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): 0\n",
      "  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): ‚úÖ S√≠\n",
      "  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\n",
      "  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\n",
      "    - segment_1: {'16': 294}\n",
      "    - segment_2: {'01': 80, '09': 38, '03': 32, '07': 19, '16': 19}\n",
      "    - segment_3: {'0138': 3, '0139': 3, '0140': 3, '0143': 3, '0062': 3}\n",
      "    - segment_4: {'46': 294}\n",
      "\n",
      "--- An√°lisis de 'CODIGO' entre Datasets (en datos limpios) ---\n",
      "  ‚Ä¢ Total de c√≥digos limpios recolectados: 6590\n",
      "  ‚Ä¢ C√≥digos √∫nicos entre todos los datasets (limpios): 6590\n",
      "  ‚Ä¢ C√≥digos duplicados (limpios, aparecen en m√°s de un lugar o repetidos en el mismo dataset): 0\n",
      "  ‚úÖ La variable 'CODIGO' es √∫nica a trav√©s de todos los datasets combinados (despu√©s de limpieza).\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def verify_codigo_cleanliness(datasets: dict):\n",
    "\n",
    "    all_codigo_values_cleaned = []\n",
    "    codigo_summary_by_dataset_cleaned = defaultdict(dict)\n",
    "    \n",
    "    codigo_pattern = re.compile(r'(\\d{1,2})-(\\d{1,3})-(\\d{1,4})-(\\d{1,3})')\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "\n",
    "        codigo_series_cleaned = df['CODIGO'].astype(str).str.strip()\n",
    "\n",
    "        # Drop truly missing values for uniqueness/segment analysis\n",
    "        valid_codigo_cleaned = codigo_series_cleaned.dropna()\n",
    "\n",
    "        # 1. Uniqueness within dataset (on cleaned data)\n",
    "        unique_count = valid_codigo_cleaned.nunique()\n",
    "        total_valid_count = len(valid_codigo_cleaned)\n",
    "        missing_count_after_clean = codigo_series_cleaned.isnull().sum()\n",
    "        \n",
    "        duplicates_in_dataset = valid_codigo_cleaned.duplicated(keep=False).sum() # Count all occurrences of duplicates\n",
    "\n",
    "        codigo_summary_by_dataset_cleaned[name]['total_entries_original'] = len(df)\n",
    "        codigo_summary_by_dataset_cleaned[name]['missing_count'] = missing_count_after_clean\n",
    "        codigo_summary_by_dataset_cleaned[name]['missing_percentage'] = (missing_count_after_clean / len(df) * 100) if len(df) > 0 else 0\n",
    "        codigo_summary_by_dataset_cleaned[name]['unique_count'] = unique_count\n",
    "        codigo_summary_by_dataset_cleaned[name]['duplicates_count'] = duplicates_in_dataset\n",
    "        codigo_summary_by_dataset_cleaned[name]['is_unique_within_dataset'] = (duplicates_in_dataset == 0) and (missing_count_after_clean == 0)\n",
    "\n",
    "        # Collect all cleaned codes for cross-dataset analysis\n",
    "        all_codigo_values_cleaned.extend(valid_codigo_cleaned.tolist())\n",
    "\n",
    "        # 2. Format Consistency (after stripping) and Segment Analysis\n",
    "        # Any remaining format violations here are structural, not just spaces\n",
    "        format_violations_cleaned = []\n",
    "        segment_counts_cleaned = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for code in valid_codigo_cleaned:\n",
    "            match = codigo_pattern.match(code)\n",
    "            if not match:\n",
    "                format_violations_cleaned.append(code)\n",
    "            else:\n",
    "                parts = match.groups()\n",
    "                segment_counts_cleaned['segment_1'][parts[0]] += 1\n",
    "                segment_counts_cleaned['segment_2'][parts[1]] += 1\n",
    "                segment_counts_cleaned['segment_3'][parts[2]] += 1\n",
    "                segment_counts_cleaned['segment_4'][parts[3]] += 1\n",
    "\n",
    "        codigo_summary_by_dataset_cleaned[name]['format_violations_cleaned'] = format_violations_cleaned\n",
    "        codigo_summary_by_dataset_cleaned[name]['segment_analysis_cleaned'] = {\n",
    "            segment: dict(sorted(counts.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "            for segment, counts in segment_counts_cleaned.items()\n",
    "        }\n",
    "        \n",
    "    print(\"\\n--- Resumen de Limpieza de 'CODIGO' por Dataset ---\")\n",
    "    for name, summary in codigo_summary_by_dataset_cleaned.items():\n",
    "        print(f\"\\nDataset: '{name}'\")\n",
    "        print(f\"  ‚Ä¢ Total entradas (original): {summary['total_entries_original']}\")\n",
    "        print(f\"  ‚Ä¢ Valores faltantes (despu√©s de limpieza): {summary['missing_count']} ({summary['missing_percentage']:.2f}%)\")\n",
    "        print(f\"  ‚Ä¢ Valores √∫nicos (despu√©s de limpieza): {summary['unique_count']}\")\n",
    "        print(f\"  ‚Ä¢ Entradas duplicadas (despu√©s de limpieza): {summary['duplicates_count']}\")\n",
    "        print(f\"  ‚Ä¢ Es √∫nico en este dataset (sin duplicados/faltantes): {'‚úÖ S√≠' if summary['is_unique_within_dataset'] else '‚ùå No'}\")\n",
    "        \n",
    "        if summary['format_violations_cleaned']:\n",
    "            print(f\"  ‚Ä¢ Violaciones de formato (despu√©s de limpieza, {len(summary['format_violations_cleaned'])}): {summary['format_violations_cleaned'][:5]}{'...' if len(summary['format_violations_cleaned']) > 5 else ''}\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ Consistencia de formato (despu√©s de limpieza): ‚úÖ Formato esperado (XX-YY-ZZZZ-AA) en todas las entradas v√°lidas.\")\n",
    "\n",
    "        print(\"  ‚Ä¢ An√°lisis de segmentos (Top 5 en datos limpios):\")\n",
    "        for segment_name, top_parts in summary['segment_analysis_cleaned'].items():\n",
    "            print(f\"    - {segment_name}: {top_parts}\")\n",
    "\n",
    "    # 3. Cross-Dataset Uniqueness and Overlap (on cleaned data)\n",
    "    print(\"\\n--- An√°lisis de 'CODIGO' entre Datasets (en datos limpios) ---\")\n",
    "    if all_codigo_values_cleaned:\n",
    "        total_codes_across_datasets = len(all_codigo_values_cleaned)\n",
    "        unique_codes_across_datasets = pd.Series(all_codigo_values_cleaned).nunique()\n",
    "        duplicates_across_datasets = total_codes_across_datasets - unique_codes_across_datasets\n",
    "\n",
    "        print(f\"  ‚Ä¢ Total de c√≥digos limpios recolectados: {total_codes_across_datasets}\")\n",
    "        print(f\"  ‚Ä¢ C√≥digos √∫nicos entre todos los datasets (limpios): {unique_codes_across_datasets}\")\n",
    "        print(f\"  ‚Ä¢ C√≥digos duplicados (limpios, aparecen en m√°s de un lugar o repetidos en el mismo dataset): {duplicates_across_datasets}\")\n",
    "        \n",
    "        if duplicates_across_datasets > 0:\n",
    "            print(\"  ‚Ä¢ La variable 'CODIGO' NO es √∫nica a trav√©s de todos los datasets combinados (despu√©s de limpieza).\")\n",
    "            duplicate_series = pd.Series(all_codigo_values_cleaned)\n",
    "            global_duplicates = duplicate_series[duplicate_series.duplicated(keep='first')].unique().tolist()\n",
    "            if global_duplicates:\n",
    "                print(f\"  ‚Ä¢ Ejemplos de c√≥digos duplicados globalmente (limpios): {global_duplicates[:10]}{'...' if len(global_duplicates) > 10 else ''}\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ La variable 'CODIGO' es √∫nica a trav√©s de todos los datasets combinados (despu√©s de limpieza).\")\n",
    "    else:\n",
    "        print(\"  No se encontraron c√≥digos 'CODIGO' v√°lidos para analizar globalmente despu√©s de la limpieza.\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "verify_codigo_cleanliness(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709841db",
   "metadata": {},
   "source": [
    "Respondiendo a las preguntas anteriores\n",
    "- Unicidad: Todos los c√≥digos dentro del dataset son √∫nicos.\n",
    "- Formato: El formato es consistente, sin embargo la funci√≥n de por si realiz√≥ algunas operaciones de limpieza como remover los espacios en blanco.\n",
    "- Valores Faltantes: No existen valores faltantes\n",
    "\n",
    "Adicionalmente, podemos seguir explorando para determinar si el segmento 2 tiene alguna otra propiedad. Este se parece repetir a lo largo de diferentes departamentos, podr√≠a ser indicador del municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52073fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Dataset 'datos_jalapa': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_jutiapa': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_peten': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_quetzaltenango': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_quiche': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_retalhuleu': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_sacatepequez': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_san_marcos': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_santa_rosa': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_solola': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_baja_verapaz': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_suchitepequez': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_totonicapan': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_zacapa': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_chimaltenango': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_chiquimula': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_ciudad_capital': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_el_progreso': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_escuintla': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_guatemala': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_huehuetenango': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_izabal': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "  ‚úÖ Dataset 'datos_alta_verapaz': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\n",
      "\n",
      "======================================================================\n",
      "Validaci√≥n individual de CODIGO (Segmento 2) vs MUNICIPIO completada y todos los datasets son consistentes\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def validate_codigo_municipio_mapping_individual(datasets: dict):\n",
    "\n",
    "    codigo_pattern = re.compile(r'(\\d{1,2})-(\\d{1,3})-(\\d{1,4})-(\\d{1,3})')\n",
    "    all_datasets_consistent = True # Flag to track overall consistency\n",
    "\n",
    "    for dataset_name, df in datasets.items():\n",
    "\n",
    "        # Extract Segment 2 and clean MUNICIPIO name\n",
    "        temp_df = df[['CODIGO', 'MUNICIPIO']].copy()\n",
    "        temp_df['CODIGO_STRIP'] = temp_df['CODIGO'].astype(str).str.strip()\n",
    "        temp_df['MUNICIPIO_CLEAN'] = temp_df['MUNICIPIO'].astype(str).str.strip().str.upper() \n",
    "\n",
    "        # Filter out malformed/missing CODIGO entries for mapping logic\n",
    "        valid_codigo_rows = temp_df['CODIGO_STRIP'].str.match(codigo_pattern).fillna(False)\n",
    "        temp_df_valid = temp_df[valid_codigo_rows].copy()\n",
    "\n",
    "        if temp_df_valid.empty:\n",
    "            print(f\"  Dataset '{dataset_name}': No hay entradas de 'CODIGO' v√°lidas para analizar la relaci√≥n con 'MUNICIPIO'.\")\n",
    "            all_datasets_consistent = False\n",
    "            continue\n",
    "\n",
    "        temp_df_valid['CODIGO_SEGMENT2'] = temp_df_valid['CODIGO_STRIP'].str.extract(codigo_pattern)[1] \n",
    "\n",
    "        # Build mappings\n",
    "        municipio_to_segment2 = temp_df_valid.groupby('MUNICIPIO_CLEAN')['CODIGO_SEGMENT2'].agg(lambda x: tuple(sorted(x.unique())))\n",
    "        segment2_to_municipio = temp_df_valid.groupby('CODIGO_SEGMENT2')['MUNICIPIO_CLEAN'].agg(lambda x: tuple(sorted(x.unique())))\n",
    "\n",
    "        # Identify inconsistencies\n",
    "        issues_in_current_dataset = []\n",
    "\n",
    "        # Check 1: Municipio name associated with multiple Segment 2 codes (less common for clean data)\n",
    "        for municipio, segment_codes in municipio_to_segment2.items():\n",
    "            if len(segment_codes) > 1:\n",
    "                issues_in_current_dataset.append(\n",
    "                    f\"  - Municipio '{municipio}' asociado a m√∫ltiples c√≥digos de segmento 2: {segment_codes}\"\n",
    "                )\n",
    "        \n",
    "        # Check 2: Segment 2 code associated with multiple Municipio names\n",
    "        for segment_code, municipios in segment2_to_municipio.items():\n",
    "            if len(municipios) > 1:\n",
    "                issues_in_current_dataset.append(\n",
    "                    f\"  - C√≥digo de segmento 2 '{segment_code}' asociado a m√∫ltiples nombres de municipio: {municipios}\"\n",
    "                )\n",
    "        \n",
    "        # Report for the current dataset\n",
    "        if issues_in_current_dataset:\n",
    "            print(f\"  ‚ùå Dataset '{dataset_name}': INCONSISTENCIAS DETECTADAS en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO:\")\n",
    "            for issue in issues_in_current_dataset[:5]: # Limit issue examples\n",
    "                print(f\"    {issue}\")\n",
    "            if len(issues_in_current_dataset) > 5:\n",
    "                print(f\"    ...y {len(issues_in_current_dataset) - 5} m√°s.\")\n",
    "            all_datasets_consistent = False\n",
    "        else:\n",
    "            # Concise success message if no issues\n",
    "            print(f\"  ‚úÖ Dataset '{dataset_name}': Consistente en la relaci√≥n CODIGO (Segmento 2) y MUNICIPIO.\")\n",
    "        \n",
    "    if all_datasets_consistent:\n",
    "        print(\"Validaci√≥n individual de CODIGO (Segmento 2) vs MUNICIPIO completada y todos los datasets son consistentes\")\n",
    "    else:\n",
    "        print(\"Validaci√≥n individual de CODIGO (Segmento 2) vs MUNICIPIO completada. Se encontraron algunas inconsistencias.\")\n",
    "\n",
    "\n",
    "validate_codigo_municipio_mapping_individual(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27583ec",
   "metadata": {},
   "source": [
    "Ahora podemos afirmar que el c√≥digo se compone de [Codigo Departamento]-[Codigo Municipio]-[Identificador]-46. En conclusi√≥n, el c√≥digo sigue un formato consistente y carece de errores de digitaci√≥n. Luego de indagar en otras variables, se puede utilizar como una \"fuente de verdad\" para arreglar errores de digitaci√≥n en otras columnas y asegurarnos que la informaci√≥n se mantenga fiel al dataset original luego de realizar m√°s operaciones de limpieza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52952f4c",
   "metadata": {},
   "source": [
    "## Pasos de Limpieza\n",
    "Ya que el c√≥digo se encuentra formateado consistentemente, la √∫nica operaci√≥n por el momento ser√≠a utilizar [Codigo Departamento]-[Codigo Municipio] para construir un identificador √∫nico de municipios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e743ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b06b38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eff77fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a4fee8",
   "metadata": {},
   "source": [
    "## Distrito"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
