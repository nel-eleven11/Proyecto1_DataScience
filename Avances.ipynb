{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1fe888",
   "metadata": {},
   "source": [
    "# Carga de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68500dc5",
   "metadata": {},
   "source": [
    "## .xls / HTML a .csv\n",
    "Los datos del Mineduc se exportan como archivos .xls, realmente teniendo contenido de HTML. Debido a eso, debemos de parsear el archivo HTML e identificar la tabla correcta que contiene los datos. Luego, podemos exportar los datos a su archivo .csv correspondiente, utilizando el valor de la columna 'Departamento' para nombrarlo. Adicionalmente, los archivos utilizan un encoding diferente al estándar utf-8, por lo cual vamos a especificarlo al momento de leer los archivos HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a36f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single file test successful!\n",
      "\n",
      "============================================================\n",
      "PROCESSING ALL FILES\n",
      "============================================================\n",
      "Found 23 .xls files to process\n",
      "  ✅ Processed establecimiento (7).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (6).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (5).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (1).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (19).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (3).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (4).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (15).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (10).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (22).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (2).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento.xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (21).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (17).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (9).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (13).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (12).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (18).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (16).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (20).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (8).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (14).xls: Created 1 CSVs.\n",
      "  ✅ Processed establecimiento (11).xls: Created 1 CSVs.\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total files: 23\n",
      "✅ Successful: 23\n",
      "❌ Failed: 0\n",
      "\n",
      "✅ No duplicate departamentos found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_html_excel_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    try:\n",
    "        tables = pd.read_html(str(file_path), encoding=\"iso-8859-1\")\n",
    "\n",
    "        if not tables:\n",
    "            return {\"success\": False, \"error\": \"No tables found in HTML\"}\n",
    "\n",
    "        required_headers = [\"CODIGO\", \"DISTRITO\", \"DEPARTAMENTO\", \"MUNICIPIO\"]\n",
    "        target_table = None\n",
    "        target_index = None\n",
    "\n",
    "        for i, df in enumerate(tables):\n",
    "            df_columns_upper = [str(col).upper().strip() for col in df.columns]\n",
    "            if all(header in df_columns_upper for header in required_headers):\n",
    "                target_table = df\n",
    "                target_index = i\n",
    "                break\n",
    "            else:\n",
    "                if len(df) > 0:\n",
    "                    first_row_upper = [\n",
    "                        str(cell).upper().strip() for cell in df.iloc[0]\n",
    "                    ]\n",
    "                    if all(header in first_row_upper for header in required_headers):\n",
    "                        df.columns = df.iloc[0]\n",
    "                        df = df.drop(df.index[0]).reset_index(drop=True)\n",
    "                        target_table = df\n",
    "                        target_index = i\n",
    "                        break\n",
    "\n",
    "        if target_table is None:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"No table found with required headers.\",\n",
    "            }\n",
    "\n",
    "        target_table = target_table.dropna(how=\"all\")\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"data\": target_table,\n",
    "            \"table_index\": target_index,\n",
    "            \"total_tables\": len(tables),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def sanitize_filename(text):\n",
    "    filename = text.lower().replace(\" \", \"_\")\n",
    "    return re.sub(r\"[^\\w_.]\", \"\", filename)\n",
    "\n",
    "\n",
    "def process_html_files_directory(input_dir, output_dir):\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    files = list(input_path.glob(\"*.xls\"))\n",
    "\n",
    "    print(f\"Found {len(files)} .xls files to process\")\n",
    "\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    all_departamentos = defaultdict(list)\n",
    "\n",
    "    for file_path in files:\n",
    "        result = parse_html_excel_file(file_path)\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            df = result[\"data\"]\n",
    "\n",
    "            departamentos = []\n",
    "            for departamento, group in df.groupby(\"DEPARTAMENTO\"):\n",
    "                filename = f\"datos_{sanitize_filename(departamento)}.csv\"\n",
    "\n",
    "                output_file = output_path / filename\n",
    "                group.to_csv(output_file, index=False)\n",
    "\n",
    "                departamentos.append(\n",
    "                    {\"name\": departamento, \"filename\": filename, \"rows\": len(group)}\n",
    "                )\n",
    "\n",
    "                all_departamentos[departamento].append(\n",
    "                    {\n",
    "                        \"source_file\": file_path.name,\n",
    "                        \"csv_file\": filename,\n",
    "                        \"rows\": len(group),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            successful_files.append(\n",
    "                {\n",
    "                    \"file\": file_path.name,\n",
    "                    \"departamentos\": departamentos,\n",
    "                    \"total_rows\": len(df),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(f\"  ✅ Processed {file_path.name}: Created {len(departamentos)} CSVs.\")\n",
    "\n",
    "        else:\n",
    "            failed_files.append({\"file\": file_path.name, \"error\": result[\"error\"]})\n",
    "            print(f\"  ❌ Failed {file_path.name}: {result['error']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nTotal files: {len(files)}\")\n",
    "    print(f\"✅ Successful: {len(successful_files)}\")\n",
    "    print(f\"❌ Failed: {len(failed_files)}\")\n",
    "\n",
    "    # The detailed successful/failed files lists will only be printed if there are successful/failed files.\n",
    "    # The summary already gives a count, so the repetition for successful files is removed here.\n",
    "    # The detailed list for failed files remains as it provides useful error messages.\n",
    "    if failed_files:\n",
    "        print(f\"\\n❌ FAILED FILES:\")\n",
    "        for item in failed_files:\n",
    "            print(f\"  - {item['file']}: {item['error']}\")\n",
    "\n",
    "    duplicates = {\n",
    "        name: sources\n",
    "        for name, sources in all_departamentos.items()\n",
    "        if len(sources) > 1\n",
    "    }\n",
    "    if duplicates:\n",
    "        print(f\"\\n⚠️ DUPLICATE DEPARTAMENTOS:\")\n",
    "        for dept_name, sources in duplicates.items():\n",
    "            print(f\"  {dept_name}: appears in {len(sources)} files\")\n",
    "    else:\n",
    "        print(\"\\n✅ No duplicate departamentos found\")\n",
    "\n",
    "\n",
    "test_result = parse_html_excel_file(\"data/raw/establecimiento.xls\")\n",
    "if test_result[\"success\"]:\n",
    "    print(\"\\nSingle file test successful!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROCESSING ALL FILES\")\n",
    "    print(\"=\" * 60)\n",
    "    process_html_files_directory(\"data/raw\", \"data/csv\")\n",
    "else:\n",
    "    print(f\"❌ Single file test failed: {test_result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e96a1",
   "metadata": {},
   "source": [
    "## .csv a DataFrames\n",
    "Luego de haber creado los archivos, podemos cargarlos a DataFrames para realizar el análisis necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bd3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 CSV files\n",
      "Loaded 23 datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load all CSV files\n",
    "csv_dir = Path(\"data/csv\")\n",
    "csv_files = list(csv_dir.glob(\"*.csv\"))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {}\n",
    "for csv_file in csv_files:\n",
    "    dataset_name = csv_file.stem\n",
    "    datasets[dataset_name] = pd.read_csv(csv_file)\n",
    "    \n",
    "print(f\"Loaded {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9176a2",
   "metadata": {},
   "source": [
    "# Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad22cb",
   "metadata": {},
   "source": [
    "## Filas y Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09701424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset Shapes:\n",
      "                 dataset  rows  columns\n",
      "0        datos_guatemala  1036       17\n",
      "12  datos_ciudad_capital   864       17\n",
      "16      datos_san_marcos   431       17\n",
      "17       datos_escuintla   393       17\n",
      "5   datos_quetzaltenango   365       17\n",
      "7    datos_chimaltenango   300       17\n",
      "11   datos_suchitepequez   296       17\n",
      "14         datos_jutiapa   296       17\n",
      "3    datos_huehuetenango   295       17\n",
      "9     datos_alta_verapaz   294       17\n",
      "18          datos_izabal   273       17\n",
      "22      datos_retalhuleu   272       17\n",
      "19           datos_peten   270       17\n",
      "21    datos_sacatepequez   208       17\n",
      "4           datos_quiche   184       17\n",
      "2       datos_chiquimula   136       17\n",
      "10      datos_santa_rosa   133       17\n",
      "15          datos_jalapa   121       17\n",
      "20          datos_solola   111       17\n",
      "8      datos_el_progreso    97       17\n",
      "1     datos_baja_verapaz    94       17\n",
      "6           datos_zacapa    70       17\n",
      "13     datos_totonicapan    51       17\n",
      "\n",
      "📈 Summary:\n",
      "Total rows across all datasets: 6,590\n",
      "Average rows per dataset: 287\n",
      "Min/Max rows: 51 / 1036\n"
     ]
    }
   ],
   "source": [
    "shape_info = []\n",
    "for name, df in datasets.items():\n",
    "    shape_info.append({\n",
    "        'dataset': name,\n",
    "        'rows': df.shape[0],\n",
    "        'columns': df.shape[1]\n",
    "    })\n",
    "\n",
    "shape_df = pd.DataFrame(shape_info)\n",
    "print(\"📊 Dataset Shapes:\")\n",
    "print(shape_df.sort_values('rows', ascending=False))\n",
    "\n",
    "print(f\"\\n📈 Summary:\")\n",
    "print(f\"Total rows across all datasets: {shape_df['rows'].sum():,}\")\n",
    "print(f\"Average rows per dataset: {shape_df['rows'].mean():.0f}\")\n",
    "print(f\"Min/Max rows: {shape_df['rows'].min()} / {shape_df['rows'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd8fd4",
   "metadata": {},
   "source": [
    "## Integridad de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fef65",
   "metadata": {},
   "source": [
    "### Consistencia en Nombres de Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e31a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Standard columns (17):\n",
      "   1. CODIGO\n",
      "   2. DISTRITO\n",
      "   3. DEPARTAMENTO\n",
      "   4. MUNICIPIO\n",
      "   5. ESTABLECIMIENTO\n",
      "   6. DIRECCION\n",
      "   7. TELEFONO\n",
      "   8. SUPERVISOR\n",
      "   9. DIRECTOR\n",
      "  10. NIVEL\n",
      "  11. SECTOR\n",
      "  12. AREA\n",
      "  13. STATUS\n",
      "  14. MODALIDAD\n",
      "  15. JORNADA\n",
      "  16. PLAN\n",
      "  17. DEPARTAMENTAL\n"
     ]
    }
   ],
   "source": [
    "all_columns = []\n",
    "column_consistency = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    columns = list(df.columns)\n",
    "    all_columns.append(columns)\n",
    "    column_consistency[name] = columns\n",
    "\n",
    "first_columns = all_columns[0]\n",
    "all_same = all(columns == first_columns for columns in all_columns)\n",
    "\n",
    "if all_same:\n",
    "    print(f\"\\n✅ Standard columns ({len(first_columns)}):\")\n",
    "    for i, col in enumerate(first_columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "else:\n",
    "    print(\"\\n❌ Column differences found:\")\n",
    "    for name, columns in column_consistency.items():\n",
    "        if columns != first_columns:\n",
    "            print(f\"  {name}: {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54d5d4",
   "metadata": {},
   "source": [
    "### Encoding Problemático\n",
    "Como mencionamos anteriormente, el encoding de los archivos originales era distinto de \"utf-8\". Nos dimos cuenta al realizar el análisis sobre el encoding problemático, sin embargo al cambiarlo dentro de la función anterior logramos correr con éxito este análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330170f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No encoding issues found (char: '�')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "problematic_char = \"�\"\n",
    "all_problematic_samples = defaultdict(list)\n",
    "issue_found = False\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            str_series = df[col].astype(str) \n",
    "            \n",
    "            contains_char_mask = str_series.str.contains(problematic_char, na=False)\n",
    "            \n",
    "            if contains_char_mask.any():\n",
    "                issue_found = True\n",
    "                current_samples = str_series[contains_char_mask].unique().tolist()\n",
    "                for sample_val in current_samples:\n",
    "                    if len(all_problematic_samples[col]) < 5:\n",
    "                        all_problematic_samples[col].append(sample_val)\n",
    "\n",
    "\n",
    "if issue_found:\n",
    "    print(f\"❌ Encoding issues found (char: '{problematic_char}'):\")\n",
    "    sorted_cols_with_issues = sorted(all_problematic_samples.keys()) \n",
    "\n",
    "    for col in sorted_cols_with_issues:\n",
    "        samples = all_problematic_samples[col]\n",
    "        print(f\"  Column '{col}':\")\n",
    "        for val in samples:\n",
    "            print(f\"    • {val}\")\n",
    "else:\n",
    "    print(f\"✅ No encoding issues found (char: '{problematic_char}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d679f59",
   "metadata": {},
   "source": [
    "# Análisis de Variables\n",
    "Las variables que más operaciones de limpieza necesitan son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1129942-f7f7-4b02-898b-85e15159e4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>missing (%)</th>\n",
       "      <th>unique_values</th>\n",
       "      <th>sample_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CODIGO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6590</td>\n",
       "      <td>[01-02-0012-46, 01-02-0013-46, 01-02-0022-46, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DISTRITO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>620</td>\n",
       "      <td>[01-502, 01-405, 01-510, 01-644, 01-645]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEPARTAMENTO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>[GUATEMALA, BAJA VERAPAZ, CHIQUIMULA, HUEHUETE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUNICIPIO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>343</td>\n",
       "      <td>[SANTA CATARINA PINULA, SAN JOSE PINULA, SAN J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESTABLECIMIENTO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3779</td>\n",
       "      <td>[LICEO INTEGRAL DE ENSEÑANZA COMERCIAL, INSTIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DIRECCION</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4428</td>\n",
       "      <td>[SECTOR LOS TRES REYES, LOTE 5\"E\" ZONA 7 ALDEA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TELEFONO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4208</td>\n",
       "      <td>[58543592, 66373741, 23660520, 66371400, 66339...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SUPERVISOR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>598</td>\n",
       "      <td>[JULIA ENECON ROCA MORAN, BLANCA SARAI GUTIERR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DIRECTOR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3860</td>\n",
       "      <td>[JOSÉ ADOLFO ESPINOZA MORATAYA, EDGAR EFRAÍN R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NIVEL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[DIVERSIFICADO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SECTOR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>[PRIVADO, OFICIAL, COOPERATIVA, MUNICIPAL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AREA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[URBANA, RURAL, SIN ESPECIFICAR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>STATUS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[ABIERTA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MODALIDAD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[MONOLINGUE, BILINGUE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JORNADA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[MATUTINA, DOBLE, VESPERTINA, SIN JORNADA, NOC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PLAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>[DIARIO(REGULAR), FIN DE SEMANA, SEMIPRESENCIA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DEPARTAMENTAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>[GUATEMALA ORIENTE, GUATEMALA NORTE, GUATEMALA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             column  missing (%)  unique_values  \\\n",
       "0            CODIGO          0.0           6590   \n",
       "1          DISTRITO          0.0            620   \n",
       "2      DEPARTAMENTO          0.0             23   \n",
       "3         MUNICIPIO          0.0            343   \n",
       "4   ESTABLECIMIENTO          0.0           3779   \n",
       "5         DIRECCION          0.0           4428   \n",
       "6          TELEFONO          0.0           4208   \n",
       "7        SUPERVISOR          0.0            598   \n",
       "8          DIRECTOR          0.0           3860   \n",
       "9             NIVEL          0.0              1   \n",
       "10           SECTOR          0.0              4   \n",
       "11             AREA          0.0              3   \n",
       "12           STATUS          0.0              1   \n",
       "13        MODALIDAD          0.0              2   \n",
       "14          JORNADA          0.0              6   \n",
       "15             PLAN          0.0             12   \n",
       "16    DEPARTAMENTAL          0.0             26   \n",
       "\n",
       "                                        sample_values  \n",
       "0   [01-02-0012-46, 01-02-0013-46, 01-02-0022-46, ...  \n",
       "1            [01-502, 01-405, 01-510, 01-644, 01-645]  \n",
       "2   [GUATEMALA, BAJA VERAPAZ, CHIQUIMULA, HUEHUETE...  \n",
       "3   [SANTA CATARINA PINULA, SAN JOSE PINULA, SAN J...  \n",
       "4   [LICEO INTEGRAL DE ENSEÑANZA COMERCIAL, INSTIT...  \n",
       "5   [SECTOR LOS TRES REYES, LOTE 5\"E\" ZONA 7 ALDEA...  \n",
       "6   [58543592, 66373741, 23660520, 66371400, 66339...  \n",
       "7   [JULIA ENECON ROCA MORAN, BLANCA SARAI GUTIERR...  \n",
       "8   [JOSÉ ADOLFO ESPINOZA MORATAYA, EDGAR EFRAÍN R...  \n",
       "9                                     [DIVERSIFICADO]  \n",
       "10         [PRIVADO, OFICIAL, COOPERATIVA, MUNICIPAL]  \n",
       "11                   [URBANA, RURAL, SIN ESPECIFICAR]  \n",
       "12                                          [ABIERTA]  \n",
       "13                             [MONOLINGUE, BILINGUE]  \n",
       "14  [MATUTINA, DOBLE, VESPERTINA, SIN JORNADA, NOC...  \n",
       "15  [DIARIO(REGULAR), FIN DE SEMANA, SEMIPRESENCIA...  \n",
       "16  [GUATEMALA ORIENTE, GUATEMALA NORTE, GUATEMALA...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_stats = []\n",
    "\n",
    "for col in first_columns:\n",
    "    col_data = []\n",
    "    for name, df in datasets.items():\n",
    "        if col in df.columns:\n",
    "            series = df[col].astype(str).str.strip()\n",
    "            col_data.extend(series)\n",
    "\n",
    "    series_all = pd.Series(col_data)\n",
    "    n_total = len(series_all)\n",
    "    n_missing = (series_all == \"\").sum() + series_all.isna().sum()\n",
    "    n_unique = series_all.nunique()\n",
    "    \n",
    "    summary_stats.append({\n",
    "        \"column\": col,\n",
    "        \"missing (%)\": round((n_missing / n_total) * 100, 2),\n",
    "        \"unique_values\": n_unique,\n",
    "        \"sample_values\": series_all.dropna().unique()[:5].tolist()\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "df_summary.sort_values(\"missing (%)\", ascending=False, inplace=True)\n",
    "\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8735b0",
   "metadata": {},
   "source": [
    "## Código\n",
    "Este valor parece ser un identificador único, queremos explorar las siguientes propiedades:\n",
    "- Unicidad: Este código es único dentro de su respectivo dataset o todos?\n",
    "- Formato: Es consistente el formato en todos los datasets? Existen errores de digitación?\n",
    "- Valores Faltantes: Existen valores faltantes?\n",
    "- Nos ayuda a identificar únicamente algún otro valor?\n",
    "\n",
    "Debido a esto, queremos realizar los siguientes pasos de limpieza:\n",
    "\n",
    "- Identificar unicidad del código\n",
    "- Identificar Valores Faltantes\n",
    "- Revisar errores de digitación, cómo lo pueden ser whitespaces o formato incongruente\n",
    "- Identificar si nos puede ayudar a identificar otras variables para verificar errores de digitación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4fee8",
   "metadata": {},
   "source": [
    "## Distrito\n",
    "Este valor parece ser un identificador geográfico, siguiendo un formato XX-YYY. Queremos explorar las siguientes propiedades\n",
    "- Formato: Es consistente el formato?\n",
    "- Valores Faltantes: Existen valores faltantes dentro de los datasets?\n",
    "- Nos ayuda a identificar únicamente algún otro valor?\n",
    "\n",
    "Debido a esto, queremos realizar los siguientes pasos de limpieza:\n",
    "\n",
    "- Identificar valores faltantes\n",
    "- Revisar errores de digitación\n",
    "- Enforzar un formato consistente\n",
    "- Identificar si nos puede ayuda a verificar la consistencia de Municipio o algún otro valor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5415d62",
   "metadata": {},
   "source": [
    "## Departamento\n",
    "\n",
    "Ya que los DataFrames se encuentran divididos por departamento, esta entrada debería ser completamente consistente. Además, podemos proponer los siguientes pasos para una mayor consistencia:\n",
    "\n",
    "- Identificar el valor RAW más común, ya que un error de digitación sería menos frecuente\n",
    "- Tomar ese valor y realizar las siguientes transformaciones\n",
    "    - Conversión a minúsculas\n",
    "    - Reemplazo de espacios por _\n",
    "    - Aplicar a todas las columnas de cada DF individual\n",
    "- Aplicar OHE luego de mergear los DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e2a71",
   "metadata": {},
   "source": [
    "## Municipio\n",
    "\n",
    "Este valor representa la división política a nivel municipal. Dado que se usa como categoría geográfica clave, es esencial asegurar consistencia.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "- Formato: ¿Se encuentra todo en mayúsculas? ¿Hay tildes inconsistentes?\n",
    "- Valores Faltantes: Confirmar si hay celdas vacías o marcadas incorrectamente.\n",
    "- Relación con otros campos: ¿El municipio concuerda con el departamento correspondiente?\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Convertir todo el texto a mayúsculas (.str.upper()).\n",
    "- Eliminar espacios extra antes o después (.str.strip()).\n",
    "- Normalizar tildes y caracteres especiales si necesario.\n",
    "- Validar los municipios contra una lista oficial por departamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddb656",
   "metadata": {},
   "source": [
    "## Establecimiento\n",
    "\n",
    "Nombre propio de la institución educativa. Puede contener muchas variantes tipográficas y estilísticas que dificultan análisis posteriores.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "- Formato: ¿Se mantiene una capitalización consistente?\n",
    "- Valores Faltantes: ¿Hay registros sin nombre?\n",
    "- Redundancia o duplicación: ¿Existen nombres duplicados con leves diferencias?\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Capitalizar nombres con .str.title() para uniformidad visual.\n",
    "- Eliminar espacios repetidos entre palabras.\n",
    "- Remover puntuación innecesaria.\n",
    "- Establecer reglas para abreviaciones comunes si se encuentran (ej. \"Inst.\" por \"Instituto\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0133d64-4ad1-4159-83eb-9c65b77ad67f",
   "metadata": {},
   "source": [
    "## DIRECCIÓN\n",
    "\n",
    "Campo libre con alta variabilidad. Las direcciones pueden contener múltiples abreviaturas, puntuación, y errores de digitación.\n",
    "\n",
    "Exploración a realizar:\n",
    "- Formato: ¿Existen patrones comunes? ¿Se usan abreviaturas (CALLE, AV, etc.)?\n",
    "- Valores inconsistentes: ¿Uso indistinto de mayúsculas, puntuación, acentos?\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "- Normalizar a mayúsculas (.str.upper()).\n",
    "- Crear reglas de sustitución para abreviaturas frecuentes (ej. AVENIDA → AV.).\n",
    "- Quitar símbolos innecesarios y estandarizar signos de puntuación.\n",
    "- Opcional: usar expresiones regulares para separar partes de la dirección (calle, número, zona, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bff941-e337-4303-aa50-85cb514fc243",
   "metadata": {},
   "source": [
    "## TELÉFONO\n",
    "\n",
    "Campo numérico con alta variabilidad en formato. Puede contener números concatenados, separadores o caracteres no numéricos.\n",
    "\n",
    "Exploración a realizar:\n",
    "- Formato: ¿Todos los valores contienen exactamente 8 dígitos? ¿Hay múltiples números por celda?\n",
    "- Errores: ¿Caracteres no numéricos? ¿Espacios o signos innecesarios?\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Remover todos los caracteres no numéricos con re.sub(r\"\\D\", \"\", telefono).\n",
    "- Validar longitud estándar (8 dígitos en Guatemala).\n",
    "- Si hay múltiples números, dividir y elegir el primero o guardarlos como lista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4968f088-68f0-45d5-8365-9ab8c5208b4d",
   "metadata": {},
   "source": [
    "## SUPERVISOR\n",
    "\n",
    "Nombre del supervisor responsable. Puede presentar variaciones por uso de tildes, mayúsculas, y errores ortográficos menores.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "- Formato: ¿Mayúsculas/minúsculas inconsistentes?\n",
    "- Duplicación: ¿Un mismo nombre aparece escrito de múltiples formas?\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Capitalizar con .str.title() para uniformidad.\n",
    "- Remover espacios dobles y caracteres extraños.\n",
    "- Normalizar tildes si es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e67f16-f9a8-4fdc-9a0f-5e188d88a914",
   "metadata": {},
   "source": [
    "## DIRECTOR\n",
    "\n",
    "Similar al campo de supervisor, representa nombres propios con riesgos similares de inconsistencia.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "- Aplicar los mismos criterios que SUPERVISOR.\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Igual estrategia de capitalización, limpieza de espacios, y normalización de caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8203578-43ba-44f3-b112-e7e765512022",
   "metadata": {},
   "source": [
    "## NIVEL\n",
    "Este campo tiene un único valor: \"DIVERSIFICADO\".\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "- Verificar que efectivamente todos los valores son iguales.\n",
    "- Confirmar si es útil conservar esta columna.\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Si es redundante, considerar eliminarla para evitar ruido en análisis futuros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b7908-d5b2-4bcf-8dba-479bc1d64b9f",
   "metadata": {},
   "source": [
    "## SECTOR\n",
    "\n",
    "Categoría con pocos valores únicos. Se espera valores como \"OFICIAL\", \"PRIVADO\", etc.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "- ¿Mayúsculas o minúsculas inconsistentes?\n",
    "- ¿Errores ortográficos?\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Convertir todo a mayúsculas y eliminar espacios (.str.upper().str.strip()).\n",
    "- Validar los valores contra un conjunto permitido: {OFICIAL, PRIVADO, MUNICIPAL, COOPERATIVA}.\n",
    "- Convertir a tipo Categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5b092-16d1-4790-be2e-4976d8533f41",
   "metadata": {},
   "source": [
    "## ÁREA\n",
    "Identifica si la institución está en zona rural o urbana.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "- Confirmar que los valores son: URBANA, RURAL, SIN ESPECIFICAR.\n",
    "- Verificar errores de digitación o combinaciones no válidas.\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "- Uniformar mayúsculas (.str.upper()).\n",
    "- Reemplazar variantes de “sin especificar” por un valor estándar (ej. \"DESCONOCIDO\").\n",
    "- Convertir a Categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb68b7",
   "metadata": {},
   "source": [
    "## STATUS\n",
    "Campo con un único valor: \"ABIERTA\".\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "Verificar si realmente todos los valores son iguales.\n",
    "\n",
    "Evaluar su utilidad en análisis futuros.\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "Eliminar si es redundante (sin variabilidad)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4aaf2",
   "metadata": {},
   "source": [
    "## MODALIDAD\n",
    "Pocos valores únicos: \"MONOLINGUE\", \"BILINGUE\".\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "Verificar mayúsculas y tildes.\n",
    "\n",
    "Validar que no existan variantes escritas incorrectamente.\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "Uniformar mayúsculas y acentos (.str.upper()).\n",
    "\n",
    "Reemplazar variantes con un mapeo fijo.\n",
    "\n",
    "Convertir a Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c401d",
   "metadata": {},
   "source": [
    "## JORNADA\n",
    "Categoría horaria. Existen valores como “MATUTINA”, “VESPERTINA”, “DOBLE”, “NOCHE”, etc.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "Verificar consistencia de términos.\n",
    "\n",
    "Identificar redundancias o términos similares con diferencias menores.\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "Normalizar formato (.str.upper().str.strip()).\n",
    "\n",
    "Mapear variantes a un conjunto estándar.\n",
    "\n",
    "Eliminar signos extra o abreviaciones inconsistentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070a90f",
   "metadata": {},
   "source": [
    "## PLAN\n",
    "Puede incluir valores con paréntesis, como “DIARIO(REGULAR)”, que dificultan análisis.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "¿Hay signos innecesarios como paréntesis o guiones?\n",
    "\n",
    "¿Hay términos redundantes?\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "Eliminar paréntesis y su contenido con str.replace(r'\\(.*?\\)', '').\n",
    "\n",
    "Eliminar espacios extra y convertir a mayúsculas.\n",
    "\n",
    "Validar valores contra una lista limpia predefinida.\n",
    "\n",
    "Convertir a Categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b50ae",
   "metadata": {},
   "source": [
    "## DEPARTAMENTAL\n",
    "Representa una división regional administrativa.\n",
    "\n",
    "Exploración a realizar:\n",
    "\n",
    "Verificar mayúsculas, errores tipográficos.\n",
    "\n",
    "Confirmar que corresponde con el valor del campo DEPARTAMENTO.\n",
    "\n",
    "Pasos de limpieza propuestos:\n",
    "\n",
    "Uniformar texto (.str.upper().str.strip()).\n",
    "\n",
    "Validar contra un listado oficial del MINEDUC.\n",
    "\n",
    "Opcional: cruzar con DEPARTAMENTO para consistencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
